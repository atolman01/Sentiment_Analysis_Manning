{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluating_Classification_Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpynpJwXeX2PFIaBgkLWJL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atolman01/Sentiment_Analysis_Manning/blob/main/Evaluating_Classification_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPhIKoin0Xrf"
      },
      "source": [
        "###Evaluating Classification Models \r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "**Binary Classification:** *is a powerful Machine Learning method that predicts positive/negative outcomes based on multiple factors or variables.*\r\n",
        "\r\n",
        "    Examples of binary classification:\r\n",
        "        Detection of diseases\r\n",
        "        survival predictions\r\n",
        "\r\n",
        "It is known as **bi**nary classicification because there is only two classes the prediction can be classified to. Emails are a great example of binary classification because they can be classified as spam or not spam.\r\n",
        "\r\n",
        "**A classifier utilizes training data to understand and relate input variables to a class**\r\n",
        "\r\n",
        "    So we can use our email classifier and it will classify unknown emails either \r\n",
        "    into the spam class or not spam class.\r\n",
        "\r\n",
        "    Also another way to think about it is an individual in the military, \r\n",
        "    there's different classes they are assigned to such as \r\n",
        "    Top Secret, Classified, Public, etc\r\n",
        "\r\n",
        "To train our classifier we need **KNOWN** spam and not spam emails. Once our email classifier is trained accurately, it can be used to classify unknown emails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx56ghFC0Xjx"
      },
      "source": [
        "https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JqS70HWem_8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAKmZPXw0Xc8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRSIHEiOzmcZ",
        "outputId": "4d46aea7-1e65-40c0-9049-21c7ba9534d3"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmbKxyEva2g0"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import altair as alt\r\n",
        "\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, recall_score\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "m0mJ7oIEai81",
        "outputId": "c6289451-3724-4ec9-a6b9-c0b6a1796b21"
      },
      "source": [
        "with open ('/content/drive/My Drive/NLP Python/Data/dictionary_sentiment.csv','r') as read_file:\r\n",
        "  sentiments = pd.read_csv(read_file)\r\n",
        "\r\n",
        "sentiments.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>rating</th>\n",
              "      <th>review</th>\n",
              "      <th>review dictionary sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4495</th>\n",
              "      <td>4495</td>\n",
              "      <td>5.0</td>\n",
              "      <td>I love this game so much that I wanted to live...</td>\n",
              "      <td>0.010417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4496</th>\n",
              "      <td>4496</td>\n",
              "      <td>5.0</td>\n",
              "      <td>this is a great game</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4497</th>\n",
              "      <td>4497</td>\n",
              "      <td>5.0</td>\n",
              "      <td>I love shooters where you're not some green ar...</td>\n",
              "      <td>0.028482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4498</th>\n",
              "      <td>4498</td>\n",
              "      <td>5.0</td>\n",
              "      <td>LBP is a fun platformer that even my wife can'...</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4499</th>\n",
              "      <td>4499</td>\n",
              "      <td>5.0</td>\n",
              "      <td>It is a blast.\\nKind of repetitive after like ...</td>\n",
              "      <td>0.015152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...  review dictionary sentiment\n",
              "4495        4495  ...                     0.010417\n",
              "4496        4496  ...                     0.200000\n",
              "4497        4497  ...                     0.028482\n",
              "4498        4498  ...                     0.033333\n",
              "4499        4499  ...                     0.015152\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJGRx0zobF69"
      },
      "source": [
        "#change from sentiment scores into sentiment rankings (positive,negative,neutral)\r\n",
        "def sentiment_score_to_value(score):\r\n",
        "  if score > 0.2:\r\n",
        "    return \"positive\"\r\n",
        "  elif score < -0.2:\r\n",
        "    return \"negative\"\r\n",
        "  else:\r\n",
        "    return \"neutral\"\r\n",
        "\r\n",
        "def rating_class(rating):\r\n",
        "  if rating == 5:\r\n",
        "    return \"positive\"\r\n",
        "  if rating == 1:\r\n",
        "    return \"negative\"\r\n",
        "  if rating in [2,3,4]:\r\n",
        "    return \"neutral\"\r\n",
        "\r\n",
        "\r\n",
        "review_sentiments = list(sentiments['review dictionary sentiment'])\r\n",
        "ratings = list(sentiments['rating'])\r\n",
        "sentiment_classes = [sentiment_score_to_value(score) for score in review_sentiments]\r\n",
        "sentiments['sentiment class'] = sentiment_classes\r\n",
        "rate_classes = [rating_class(r) for r in ratings]\r\n",
        "sentiments['rating class'] = rate_classes\r\n",
        "\r\n",
        "sentiments.to_csv('/content/drive/My Drive/NLP Python/Data/evaluate_sentiment.csv')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRqYwzlGzrjA"
      },
      "source": [
        "How many reviews are categorized correctly as positive, negative, or neutral by your dictionary-based sentiment analyzer? (This value is called accuracy.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXq5lCNdzs-_",
        "outputId": "917b56b5-26c0-4c38-ad01-00079c317275"
      },
      "source": [
        "# the predicted values are the sentiment values \r\n",
        "predicted = list(sentiments['sentiment class'])\r\n",
        "\r\n",
        "# the actual values are the ratings \r\n",
        "actual = list(sentiments['rating class'])\r\n",
        "\r\n",
        "accuracy_score(actual,predicted)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36977777777777776"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLlHiTEOUou6",
        "outputId": "dc7572b0-b7ed-465e-f852-cf4ebc93ad14"
      },
      "source": [
        "precision_score(actual,predicted,average='macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6215377816926315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcbAH3nLJkMF",
        "outputId": "905b1ba1-70ae-458a-f9ab-d3ae8a7a2edb"
      },
      "source": [
        "recall_score(actual,predicted,average='macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3697777777777778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7HUuJkAD7MG",
        "outputId": "e4343ee3-0331-4178-ec47-3478f1d95860"
      },
      "source": [
        "print(classification_report(actual,predicted,target_names=['negative','neutral','positive']))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.74      0.02      0.04      1500\n",
            "     neutral       0.34      0.98      0.51      1500\n",
            "    positive       0.78      0.12      0.20      1500\n",
            "\n",
            "    accuracy                           0.37      4500\n",
            "   macro avg       0.62      0.37      0.25      4500\n",
            "weighted avg       0.62      0.37      0.25      4500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}